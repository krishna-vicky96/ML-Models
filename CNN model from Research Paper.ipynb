{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U2hGuHDs1gJ"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "#from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsC51k5Ds1g6"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ICRvH11Ms1hD",
    "outputId": "b6849642-a661-4c37-db32-025847c63a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3Bo_h-MMs1hM",
    "outputId": "5ca934df-1605-42b5-a7f8-dee34dab3a28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YAF_WQvrs1hZ",
    "outputId": "9468f65b-c958-4415-9743-6155179795c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwVkLMNws1hi"
   },
   "outputs": [],
   "source": [
    "data_format = K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8QafMDf5s1hs",
    "outputId": "6b6781ed-9536-41aa-9788-5a15400261e0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.common.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Il8DDFwGs1h5",
    "outputId": "42aa6724-e481-43f8-fa5a-d70265328bdf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_wr3irVs1iC"
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kDNq1zjs1iK"
   },
   "outputs": [],
   "source": [
    "generator = ImageDataGenerator(rotation_range=15,\n",
    "                               width_shift_range=5./32,\n",
    "                               height_shift_range=5./32,\n",
    "                               horizontal_flip=True)\n",
    "\n",
    "generator.fit(X_train, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oM7drk2os1iV"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UWl3-nms1if"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 42):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization(epsilon=1.1e-5)(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), kernel_initializer =\"he_normal\", use_bias=False ,padding='same')(relu)\n",
    "        #Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 42):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization(epsilon=1.1e-5)(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), kernel_initializer =\"he_normal\", use_bias=False ,padding='same', kernel_regularizer=l2(1e-4))(relu)\n",
    "    #Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    features_total = int(flat.get_shape()[-1])\n",
    "    flat = tf.reshape(flat, [-1, features_total])\n",
    "    #print(flat.shape)\n",
    "    \n",
    "    #https://towardsdatascience.com/neural-network-with-tensorflow-how-to-stop-training-using-callback-5c8d575c18a9\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "    W = initializer(shape=(features_total, 10))\n",
    "    \n",
    "    initializer2 = tf.keras.initializers.Constant(0.0)\n",
    "    bias = initializer2(shape=(10))\n",
    "    out = tf.matmul(flat, W) + bias\n",
    "    \n",
    "    output = tf.math.exp(out) # exponentiate vector of raw predictions\n",
    "    #print(output)\n",
    "    out = tf.math.reduce_sum(output)\n",
    "    return output/out\n",
    "\n",
    "    #output = layers.Dense(10, activation='softmax')(flat)\n",
    "    #return output\n",
    "\n",
    "#output layer\n",
    "def output_layer1(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(10, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWOZ_QCQs1ip"
   },
   "outputs": [],
   "source": [
    "num_filter = 42\n",
    "l = 10\n",
    "compression = 0.5\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), activation=\"relu\", strides=(2,2), kernel_initializer =\"he_uniform\", use_bias=False ,padding='same', kernel_regularizer=l2(1e-4))(input)\n",
    "First_Conv2D = layers.BatchNormalization(axis=-1, epsilon=1.1e-5)(First_Conv2D)\n",
    "#First_Conv2D = layers.MaxPooling2D((3, 3), strides=2, padding='same',data_format='channels_last')(First_Conv2D)\n",
    "#First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter)\n",
    "First_Transition = transition(First_Block, num_filter)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter)\n",
    "Second_Transition = transition(Second_Block, num_filter)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter)\n",
    "Third_Transition = transition(Third_Block, num_filter)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter)\n",
    "output = output_layer(Last_Block)\n",
    "\n",
    "#outputs = [model(model_inputs) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bLkiQzjZs1i2",
    "outputId": "3b282648-a678-4008-947f-a30857e33a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 16, 42)   1134        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 42)   168         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 42)   168         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 16, 42)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 21)   7938        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 63)   0           batch_normalization[0][0]        \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 63)   252         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 63)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 21)   11907       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 84)   0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 84)   336         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 84)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 21)   15876       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 105)  0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 105)  420         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 105)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 21)   19845       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 126)  0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 126)  504         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 126)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 21)   23814       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 147)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 147)  588         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 147)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 21)   27783       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 168)  0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 168)  672         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 168)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 21)   31752       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 189)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 189)  756         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 189)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 21)   35721       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 210)  0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 210)  840         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 210)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 21)   39690       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 231)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 231)  924         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 231)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 21)   43659       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 252)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 252)  1008        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 252)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 21)   5292        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 8, 8, 21)     0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 21)     84          average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 21)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 21)     3969        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8, 8, 42)     0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 42)     168         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 42)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 21)     7938        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 8, 8, 63)     0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 63)     252         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 63)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 21)     11907       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8, 8, 84)     0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 84)     336         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 84)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 21)     15876       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 8, 8, 105)    0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 105)    420         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 105)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 21)     19845       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 126)    0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 126)    504         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 126)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 21)     23814       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 8, 8, 147)    0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 147)    588         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 147)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 21)     27783       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 8, 8, 168)    0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 168)    672         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 168)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 21)     31752       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 8, 8, 189)    0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 189)    756         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 189)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 21)     35721       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 210)    0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 210)    840         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 210)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 21)     39690       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 8, 8, 231)    0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 231)    924         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 231)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 21)     4851        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 4, 4, 21)     0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 21)     84          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 21)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 21)     3969        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 4, 4, 42)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 42)     168         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 42)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 4, 21)     7938        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 4, 4, 63)     0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 63)     252         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 63)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 21)     11907       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 84)     0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 84)     336         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 84)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 21)     15876       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 105)    0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 105)    420         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 105)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 21)     19845       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 4, 4, 126)    0           concatenate_23[0][0]             \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 126)    504         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 126)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 21)     23814       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 4, 4, 147)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 147)    588         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 147)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 21)     27783       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 4, 4, 168)    0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 168)    672         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 168)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 21)     31752       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 4, 4, 189)    0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 189)    756         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 189)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 21)     35721       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 4, 4, 210)    0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 210)    840         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 210)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 21)     39690       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 4, 4, 231)    0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 231)    924         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 231)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 21)     4851        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 2, 2, 21)     0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 2, 2, 21)     84          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 2, 2, 21)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 2, 2, 21)     3969        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 2, 2, 42)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2, 2, 42)     168         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2, 2, 42)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 2, 2, 21)     7938        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 2, 2, 63)     0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2, 2, 63)     252         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2, 2, 63)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 2, 2, 21)     11907       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 2, 2, 84)     0           concatenate_31[0][0]             \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2, 2, 84)     336         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 2, 2, 84)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 2, 2, 21)     15876       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 2, 2, 105)    0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 2, 105)    420         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 2, 2, 105)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 2, 2, 21)     19845       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 2, 2, 126)    0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 2, 2, 126)    504         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 2, 2, 126)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 2, 2, 21)     23814       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 2, 2, 147)    0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 2, 2, 147)    588         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 2, 2, 147)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 2, 2, 21)     27783       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 2, 2, 168)    0           concatenate_35[0][0]             \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 2, 2, 168)    672         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 168)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 2, 2, 21)     31752       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 2, 2, 189)    0           concatenate_36[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2, 2, 189)    756         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2, 2, 189)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 2, 2, 21)     35721       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 2, 2, 210)    0           concatenate_37[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 2, 2, 210)    840         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 2, 210)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 2, 2, 21)     39690       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 2, 2, 231)    0           concatenate_38[0][0]             \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 2, 2, 231)    924         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 2, 231)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 231)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 231)          0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 231)]        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul (TensorFlowO [(None, 10)]         0           tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 10)]         0           tf_op_layer_MatMul[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp (TensorFlowOpLa [(None, 10)]         0           tf_op_layer_AddV2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [()]                 0           tf_op_layer_Exp[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 10)]         0           tf_op_layer_Exp[0][0]            \n",
      "                                                                 tf_op_layer_Sum[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 952,266\n",
      "Trainable params: 940,632\n",
      "Non-trainable params: 11,634\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWver-Dns1i_"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59889940/how-to-write-custom-callback-for-saving-the-model-at-every-epoch-if-validation-a\n",
    "#https://stackoverflow.com/questions/47079111/create-keras-callback-to-save-model-predictions-and-targets-for-each-batch-durin\n",
    "\n",
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        #lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('val_accuracy') > 0.9):   \n",
    "          print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(0.9*100))   \n",
    "          self.model.stop_training = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6klo-fCks1jJ"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    lr = 0.1\n",
    "    if epoch == 150 or epoch >=290:\n",
    "        lr = 0.01\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_d7uszB5s1jU",
    "outputId": "006d2bb2-4b3d-47b8-e5b4-875eafe2aaa3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nmodel.compile(loss='categorical_crossentropy',\\n              optimizer=Adam(),\\n              metrics=['accuracy'])\""
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(momentum = 0.9, learning_rate=0.1),\n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ll1OD7sys1jd",
    "outputId": "713b6ad7-1c1a-43e2-8979-8aad03e07b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.1000.\n",
      "Epoch 1/300\n",
      "390/390 [==============================] - 28s 71ms/step - loss: 1.3153 - accuracy: 0.5326 - val_loss: 1.5850 - val_accuracy: 0.4804\n",
      "\n",
      "Epoch 00001: Learning rate is 0.1000.\n",
      "Epoch 2/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 1.1420 - accuracy: 0.6031 - val_loss: 1.9643 - val_accuracy: 0.4390\n",
      "\n",
      "Epoch 00002: Learning rate is 0.1000.\n",
      "Epoch 3/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 1.0259 - accuracy: 0.6479 - val_loss: 1.2476 - val_accuracy: 0.5942\n",
      "\n",
      "Epoch 00003: Learning rate is 0.1000.\n",
      "Epoch 4/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.9392 - accuracy: 0.6815 - val_loss: 1.0799 - val_accuracy: 0.6553\n",
      "\n",
      "Epoch 00004: Learning rate is 0.1000.\n",
      "Epoch 5/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.8690 - accuracy: 0.7078 - val_loss: 1.0564 - val_accuracy: 0.6574\n",
      "\n",
      "Epoch 00005: Learning rate is 0.1000.\n",
      "Epoch 6/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.8171 - accuracy: 0.7268 - val_loss: 0.9379 - val_accuracy: 0.6910\n",
      "\n",
      "Epoch 00006: Learning rate is 0.1000.\n",
      "Epoch 7/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.7732 - accuracy: 0.7450 - val_loss: 1.1348 - val_accuracy: 0.6554\n",
      "\n",
      "Epoch 00007: Learning rate is 0.1000.\n",
      "Epoch 8/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.7315 - accuracy: 0.7599 - val_loss: 0.9654 - val_accuracy: 0.6877\n",
      "\n",
      "Epoch 00008: Learning rate is 0.1000.\n",
      "Epoch 9/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.7013 - accuracy: 0.7700 - val_loss: 1.0906 - val_accuracy: 0.6585\n",
      "\n",
      "Epoch 00009: Learning rate is 0.1000.\n",
      "Epoch 10/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.6630 - accuracy: 0.7836 - val_loss: 0.9806 - val_accuracy: 0.6968\n",
      "\n",
      "Epoch 00010: Learning rate is 0.1000.\n",
      "Epoch 11/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.6383 - accuracy: 0.7942 - val_loss: 0.7991 - val_accuracy: 0.7360\n",
      "\n",
      "Epoch 00011: Learning rate is 0.1000.\n",
      "Epoch 12/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.6151 - accuracy: 0.8028 - val_loss: 0.8458 - val_accuracy: 0.7349\n",
      "\n",
      "Epoch 00012: Learning rate is 0.1000.\n",
      "Epoch 13/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.5988 - accuracy: 0.8079 - val_loss: 0.7922 - val_accuracy: 0.7506\n",
      "\n",
      "Epoch 00013: Learning rate is 0.1000.\n",
      "Epoch 14/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.9316 - val_accuracy: 0.7114\n",
      "\n",
      "Epoch 00014: Learning rate is 0.1000.\n",
      "Epoch 15/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.5623 - accuracy: 0.8210 - val_loss: 0.6488 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00015: Learning rate is 0.1000.\n",
      "Epoch 16/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.5459 - accuracy: 0.8246 - val_loss: 0.7555 - val_accuracy: 0.7690\n",
      "\n",
      "Epoch 00016: Learning rate is 0.1000.\n",
      "Epoch 17/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.5299 - accuracy: 0.8317 - val_loss: 0.9503 - val_accuracy: 0.7230\n",
      "\n",
      "Epoch 00017: Learning rate is 0.1000.\n",
      "Epoch 18/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.5169 - accuracy: 0.8363 - val_loss: 1.0002 - val_accuracy: 0.7064\n",
      "\n",
      "Epoch 00018: Learning rate is 0.1000.\n",
      "Epoch 19/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.5097 - accuracy: 0.8384 - val_loss: 0.8524 - val_accuracy: 0.7389\n",
      "\n",
      "Epoch 00019: Learning rate is 0.1000.\n",
      "Epoch 20/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4934 - accuracy: 0.8450 - val_loss: 0.9473 - val_accuracy: 0.7082\n",
      "\n",
      "Epoch 00020: Learning rate is 0.1000.\n",
      "Epoch 21/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.4864 - accuracy: 0.8457 - val_loss: 0.5987 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00021: Learning rate is 0.1000.\n",
      "Epoch 22/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.4740 - accuracy: 0.8506 - val_loss: 0.6348 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00022: Learning rate is 0.1000.\n",
      "Epoch 23/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4616 - accuracy: 0.8543 - val_loss: 0.6629 - val_accuracy: 0.7970\n",
      "\n",
      "Epoch 00023: Learning rate is 0.1000.\n",
      "Epoch 24/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4553 - accuracy: 0.8581 - val_loss: 1.1282 - val_accuracy: 0.6897\n",
      "\n",
      "Epoch 00024: Learning rate is 0.1000.\n",
      "Epoch 25/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.4449 - accuracy: 0.8606 - val_loss: 0.6890 - val_accuracy: 0.7887\n",
      "\n",
      "Epoch 00025: Learning rate is 0.1000.\n",
      "Epoch 26/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4416 - accuracy: 0.8625 - val_loss: 0.8334 - val_accuracy: 0.7553\n",
      "\n",
      "Epoch 00026: Learning rate is 0.1000.\n",
      "Epoch 27/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4318 - accuracy: 0.8657 - val_loss: 0.6274 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 00027: Learning rate is 0.1000.\n",
      "Epoch 28/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4251 - accuracy: 0.8679 - val_loss: 0.6112 - val_accuracy: 0.8194\n",
      "\n",
      "Epoch 00028: Learning rate is 0.1000.\n",
      "Epoch 29/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4157 - accuracy: 0.8715 - val_loss: 0.6126 - val_accuracy: 0.8129\n",
      "\n",
      "Epoch 00029: Learning rate is 0.1000.\n",
      "Epoch 30/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4102 - accuracy: 0.8725 - val_loss: 0.5539 - val_accuracy: 0.8342\n",
      "\n",
      "Epoch 00030: Learning rate is 0.1000.\n",
      "Epoch 31/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.4051 - accuracy: 0.8745 - val_loss: 0.5134 - val_accuracy: 0.8420\n",
      "\n",
      "Epoch 00031: Learning rate is 0.1000.\n",
      "Epoch 32/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3966 - accuracy: 0.8783 - val_loss: 0.5915 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00032: Learning rate is 0.1000.\n",
      "Epoch 33/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3928 - accuracy: 0.8783 - val_loss: 0.7355 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00033: Learning rate is 0.1000.\n",
      "Epoch 34/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3900 - accuracy: 0.8793 - val_loss: 0.7659 - val_accuracy: 0.7780\n",
      "\n",
      "Epoch 00034: Learning rate is 0.1000.\n",
      "Epoch 35/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3747 - accuracy: 0.8837 - val_loss: 0.6575 - val_accuracy: 0.8091\n",
      "\n",
      "Epoch 00035: Learning rate is 0.1000.\n",
      "Epoch 36/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3713 - accuracy: 0.8863 - val_loss: 0.6631 - val_accuracy: 0.8048\n",
      "\n",
      "Epoch 00036: Learning rate is 0.1000.\n",
      "Epoch 37/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3661 - accuracy: 0.8868 - val_loss: 0.5702 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00037: Learning rate is 0.1000.\n",
      "Epoch 38/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3657 - accuracy: 0.8887 - val_loss: 0.6715 - val_accuracy: 0.8015\n",
      "\n",
      "Epoch 00038: Learning rate is 0.1000.\n",
      "Epoch 39/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3616 - accuracy: 0.8895 - val_loss: 0.6745 - val_accuracy: 0.7984\n",
      "\n",
      "Epoch 00039: Learning rate is 0.1000.\n",
      "Epoch 40/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3581 - accuracy: 0.8897 - val_loss: 0.5775 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00040: Learning rate is 0.1000.\n",
      "Epoch 41/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3452 - accuracy: 0.8959 - val_loss: 0.5485 - val_accuracy: 0.8342\n",
      "\n",
      "Epoch 00041: Learning rate is 0.1000.\n",
      "Epoch 42/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3453 - accuracy: 0.8959 - val_loss: 0.4937 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00042: Learning rate is 0.1000.\n",
      "Epoch 43/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3399 - accuracy: 0.8972 - val_loss: 0.5652 - val_accuracy: 0.8313\n",
      "\n",
      "Epoch 00043: Learning rate is 0.1000.\n",
      "Epoch 44/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3429 - accuracy: 0.8961 - val_loss: 0.5202 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00044: Learning rate is 0.1000.\n",
      "Epoch 45/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3322 - accuracy: 0.8993 - val_loss: 0.6016 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00045: Learning rate is 0.1000.\n",
      "Epoch 46/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3329 - accuracy: 0.8974 - val_loss: 0.7265 - val_accuracy: 0.7946\n",
      "\n",
      "Epoch 00046: Learning rate is 0.1000.\n",
      "Epoch 47/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3262 - accuracy: 0.9003 - val_loss: 1.1106 - val_accuracy: 0.7135\n",
      "\n",
      "Epoch 00047: Learning rate is 0.1000.\n",
      "Epoch 48/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3202 - accuracy: 0.9028 - val_loss: 0.6252 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00048: Learning rate is 0.1000.\n",
      "Epoch 49/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3175 - accuracy: 0.9049 - val_loss: 0.5692 - val_accuracy: 0.8361\n",
      "\n",
      "Epoch 00049: Learning rate is 0.1000.\n",
      "Epoch 50/300\n",
      "390/390 [==============================] - 28s 71ms/step - loss: 0.3150 - accuracy: 0.9033 - val_loss: 0.5452 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00050: Learning rate is 0.1000.\n",
      "Epoch 51/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3109 - accuracy: 0.9065 - val_loss: 0.5661 - val_accuracy: 0.8347\n",
      "\n",
      "Epoch 00051: Learning rate is 0.1000.\n",
      "Epoch 52/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3097 - accuracy: 0.9063 - val_loss: 0.4839 - val_accuracy: 0.8551\n",
      "\n",
      "Epoch 00052: Learning rate is 0.1000.\n",
      "Epoch 53/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3057 - accuracy: 0.9077 - val_loss: 0.6028 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00053: Learning rate is 0.1000.\n",
      "Epoch 54/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3087 - accuracy: 0.9075 - val_loss: 0.5441 - val_accuracy: 0.8430\n",
      "\n",
      "Epoch 00054: Learning rate is 0.1000.\n",
      "Epoch 55/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3017 - accuracy: 0.9102 - val_loss: 0.7118 - val_accuracy: 0.8024\n",
      "\n",
      "Epoch 00055: Learning rate is 0.1000.\n",
      "Epoch 56/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2965 - accuracy: 0.9110 - val_loss: 0.7320 - val_accuracy: 0.8011\n",
      "\n",
      "Epoch 00056: Learning rate is 0.1000.\n",
      "Epoch 57/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2956 - accuracy: 0.9106 - val_loss: 0.7020 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00057: Learning rate is 0.1000.\n",
      "Epoch 58/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2946 - accuracy: 0.9121 - val_loss: 0.7315 - val_accuracy: 0.7936\n",
      "\n",
      "Epoch 00058: Learning rate is 0.1000.\n",
      "Epoch 59/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2969 - accuracy: 0.9096 - val_loss: 0.7946 - val_accuracy: 0.7721\n",
      "\n",
      "Epoch 00059: Learning rate is 0.1000.\n",
      "Epoch 60/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2848 - accuracy: 0.9143 - val_loss: 0.4956 - val_accuracy: 0.8601\n",
      "\n",
      "Epoch 00060: Learning rate is 0.1000.\n",
      "Epoch 61/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2847 - accuracy: 0.9148 - val_loss: 0.4363 - val_accuracy: 0.8746\n",
      "\n",
      "Epoch 00061: Learning rate is 0.1000.\n",
      "Epoch 62/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2764 - accuracy: 0.9184 - val_loss: 0.5269 - val_accuracy: 0.8512\n",
      "\n",
      "Epoch 00062: Learning rate is 0.1000.\n",
      "Epoch 63/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2822 - accuracy: 0.9143 - val_loss: 0.6386 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00063: Learning rate is 0.1000.\n",
      "Epoch 64/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2789 - accuracy: 0.9179 - val_loss: 0.6495 - val_accuracy: 0.8258\n",
      "\n",
      "Epoch 00064: Learning rate is 0.1000.\n",
      "Epoch 65/300\n",
      "390/390 [==============================] - 28s 71ms/step - loss: 0.2689 - accuracy: 0.9210 - val_loss: 0.5438 - val_accuracy: 0.8485\n",
      "\n",
      "Epoch 00065: Learning rate is 0.1000.\n",
      "Epoch 66/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2741 - accuracy: 0.9176 - val_loss: 0.5510 - val_accuracy: 0.8499\n",
      "\n",
      "Epoch 00066: Learning rate is 0.1000.\n",
      "Epoch 67/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2654 - accuracy: 0.9211 - val_loss: 0.5991 - val_accuracy: 0.8379\n",
      "\n",
      "Epoch 00067: Learning rate is 0.1000.\n",
      "Epoch 68/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2690 - accuracy: 0.9200 - val_loss: 0.6090 - val_accuracy: 0.8355\n",
      "\n",
      "Epoch 00068: Learning rate is 0.1000.\n",
      "Epoch 69/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2608 - accuracy: 0.9230 - val_loss: 0.5514 - val_accuracy: 0.8413\n",
      "\n",
      "Epoch 00069: Learning rate is 0.1000.\n",
      "Epoch 70/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2627 - accuracy: 0.9226 - val_loss: 0.6523 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00070: Learning rate is 0.1000.\n",
      "Epoch 71/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2573 - accuracy: 0.9243 - val_loss: 0.7060 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00071: Learning rate is 0.1000.\n",
      "Epoch 72/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2550 - accuracy: 0.9245 - val_loss: 0.6011 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 00072: Learning rate is 0.1000.\n",
      "Epoch 73/300\n",
      "390/390 [==============================] - 28s 71ms/step - loss: 0.2582 - accuracy: 0.9236 - val_loss: 0.4773 - val_accuracy: 0.8707\n",
      "\n",
      "Epoch 00073: Learning rate is 0.1000.\n",
      "Epoch 74/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2513 - accuracy: 0.9250 - val_loss: 1.1092 - val_accuracy: 0.7232\n",
      "\n",
      "Epoch 00074: Learning rate is 0.1000.\n",
      "Epoch 75/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2505 - accuracy: 0.9264 - val_loss: 0.8716 - val_accuracy: 0.7782\n",
      "\n",
      "Epoch 00075: Learning rate is 0.1000.\n",
      "Epoch 76/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2478 - accuracy: 0.9270 - val_loss: 0.7748 - val_accuracy: 0.7915\n",
      "\n",
      "Epoch 00076: Learning rate is 0.1000.\n",
      "Epoch 77/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2529 - accuracy: 0.9249 - val_loss: 0.9520 - val_accuracy: 0.7676\n",
      "\n",
      "Epoch 00077: Learning rate is 0.1000.\n",
      "Epoch 78/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2435 - accuracy: 0.9288 - val_loss: 0.6310 - val_accuracy: 0.8305\n",
      "\n",
      "Epoch 00078: Learning rate is 0.1000.\n",
      "Epoch 79/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2451 - accuracy: 0.9267 - val_loss: 0.5325 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00079: Learning rate is 0.1000.\n",
      "Epoch 80/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2440 - accuracy: 0.9283 - val_loss: 0.5028 - val_accuracy: 0.8575\n",
      "\n",
      "Epoch 00080: Learning rate is 0.1000.\n",
      "Epoch 81/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2406 - accuracy: 0.9292 - val_loss: 0.5296 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00081: Learning rate is 0.1000.\n",
      "Epoch 82/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2374 - accuracy: 0.9304 - val_loss: 0.6416 - val_accuracy: 0.8305\n",
      "\n",
      "Epoch 00082: Learning rate is 0.1000.\n",
      "Epoch 83/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2406 - accuracy: 0.9297 - val_loss: 0.5747 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00083: Learning rate is 0.1000.\n",
      "Epoch 84/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2326 - accuracy: 0.9329 - val_loss: 0.5477 - val_accuracy: 0.8514\n",
      "\n",
      "Epoch 00084: Learning rate is 0.1000.\n",
      "Epoch 85/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2367 - accuracy: 0.9289 - val_loss: 0.7091 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00085: Learning rate is 0.1000.\n",
      "Epoch 86/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2279 - accuracy: 0.9337 - val_loss: 0.5786 - val_accuracy: 0.8459\n",
      "\n",
      "Epoch 00086: Learning rate is 0.1000.\n",
      "Epoch 87/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2328 - accuracy: 0.9321 - val_loss: 0.5412 - val_accuracy: 0.8558\n",
      "\n",
      "Epoch 00087: Learning rate is 0.1000.\n",
      "Epoch 88/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2281 - accuracy: 0.9330 - val_loss: 0.4583 - val_accuracy: 0.8690\n",
      "\n",
      "Epoch 00088: Learning rate is 0.1000.\n",
      "Epoch 89/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2313 - accuracy: 0.9322 - val_loss: 0.5334 - val_accuracy: 0.8599\n",
      "\n",
      "Epoch 00089: Learning rate is 0.1000.\n",
      "Epoch 90/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.2226 - accuracy: 0.9359 - val_loss: 0.4658 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00090: Learning rate is 0.1000.\n",
      "Epoch 91/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.2307 - accuracy: 0.9335 - val_loss: 0.7314 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00091: Learning rate is 0.1000.\n",
      "Epoch 92/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2232 - accuracy: 0.9358 - val_loss: 0.6535 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00092: Learning rate is 0.1000.\n",
      "Epoch 93/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.2214 - accuracy: 0.9366 - val_loss: 0.6406 - val_accuracy: 0.8340\n",
      "\n",
      "Epoch 00093: Learning rate is 0.1000.\n",
      "Epoch 94/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2221 - accuracy: 0.9360 - val_loss: 0.5548 - val_accuracy: 0.8476\n",
      "\n",
      "Epoch 00094: Learning rate is 0.1000.\n",
      "Epoch 95/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2133 - accuracy: 0.9385 - val_loss: 0.5780 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00095: Learning rate is 0.1000.\n",
      "Epoch 96/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2187 - accuracy: 0.9375 - val_loss: 0.5008 - val_accuracy: 0.8616\n",
      "\n",
      "Epoch 00096: Learning rate is 0.1000.\n",
      "Epoch 97/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.2158 - accuracy: 0.9386 - val_loss: 0.4914 - val_accuracy: 0.8617\n",
      "\n",
      "Epoch 00097: Learning rate is 0.1000.\n",
      "Epoch 98/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2139 - accuracy: 0.9391 - val_loss: 0.4872 - val_accuracy: 0.8671\n",
      "\n",
      "Epoch 00098: Learning rate is 0.1000.\n",
      "Epoch 99/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2169 - accuracy: 0.9379 - val_loss: 0.5110 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00099: Learning rate is 0.1000.\n",
      "Epoch 100/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2135 - accuracy: 0.9382 - val_loss: 0.5722 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00100: Learning rate is 0.1000.\n",
      "Epoch 101/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2141 - accuracy: 0.9389 - val_loss: 0.6272 - val_accuracy: 0.8353\n",
      "\n",
      "Epoch 00101: Learning rate is 0.1000.\n",
      "Epoch 102/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2024 - accuracy: 0.9412 - val_loss: 0.5451 - val_accuracy: 0.8515\n",
      "\n",
      "Epoch 00102: Learning rate is 0.1000.\n",
      "Epoch 103/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2116 - accuracy: 0.9393 - val_loss: 0.7024 - val_accuracy: 0.8236\n",
      "\n",
      "Epoch 00103: Learning rate is 0.1000.\n",
      "Epoch 104/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2095 - accuracy: 0.9396 - val_loss: 0.6383 - val_accuracy: 0.8360\n",
      "\n",
      "Epoch 00104: Learning rate is 0.1000.\n",
      "Epoch 105/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2035 - accuracy: 0.9418 - val_loss: 0.4894 - val_accuracy: 0.8712\n",
      "\n",
      "Epoch 00105: Learning rate is 0.1000.\n",
      "Epoch 106/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1998 - accuracy: 0.9430 - val_loss: 0.4630 - val_accuracy: 0.8739\n",
      "\n",
      "Epoch 00106: Learning rate is 0.1000.\n",
      "Epoch 107/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1963 - accuracy: 0.9442 - val_loss: 0.5499 - val_accuracy: 0.8555\n",
      "\n",
      "Epoch 00107: Learning rate is 0.1000.\n",
      "Epoch 108/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.2013 - accuracy: 0.9418 - val_loss: 0.5254 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00108: Learning rate is 0.1000.\n",
      "Epoch 109/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1992 - accuracy: 0.9443 - val_loss: 0.5186 - val_accuracy: 0.8643\n",
      "\n",
      "Epoch 00109: Learning rate is 0.1000.\n",
      "Epoch 110/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2022 - accuracy: 0.9425 - val_loss: 0.5551 - val_accuracy: 0.8510\n",
      "\n",
      "Epoch 00110: Learning rate is 0.1000.\n",
      "Epoch 111/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2001 - accuracy: 0.9428 - val_loss: 0.5256 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00111: Learning rate is 0.1000.\n",
      "Epoch 112/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1984 - accuracy: 0.9435 - val_loss: 0.4211 - val_accuracy: 0.8868\n",
      "\n",
      "Epoch 00112: Learning rate is 0.1000.\n",
      "Epoch 113/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1999 - accuracy: 0.9432 - val_loss: 0.5679 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00113: Learning rate is 0.1000.\n",
      "Epoch 114/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1909 - accuracy: 0.9466 - val_loss: 0.5999 - val_accuracy: 0.8477\n",
      "\n",
      "Epoch 00114: Learning rate is 0.1000.\n",
      "Epoch 115/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1947 - accuracy: 0.9458 - val_loss: 0.4633 - val_accuracy: 0.8784\n",
      "\n",
      "Epoch 00115: Learning rate is 0.1000.\n",
      "Epoch 116/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1922 - accuracy: 0.9461 - val_loss: 0.5755 - val_accuracy: 0.8528\n",
      "\n",
      "Epoch 00116: Learning rate is 0.1000.\n",
      "Epoch 117/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1964 - accuracy: 0.9446 - val_loss: 0.5314 - val_accuracy: 0.8580\n",
      "\n",
      "Epoch 00117: Learning rate is 0.1000.\n",
      "Epoch 118/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1909 - accuracy: 0.9469 - val_loss: 0.5951 - val_accuracy: 0.8453\n",
      "\n",
      "Epoch 00118: Learning rate is 0.1000.\n",
      "Epoch 119/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1851 - accuracy: 0.9477 - val_loss: 0.5862 - val_accuracy: 0.8522\n",
      "\n",
      "Epoch 00119: Learning rate is 0.1000.\n",
      "Epoch 120/300\n",
      "390/390 [==============================] - 28s 71ms/step - loss: 0.1927 - accuracy: 0.9462 - val_loss: 0.5741 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00120: Learning rate is 0.1000.\n",
      "Epoch 121/300\n",
      "390/390 [==============================] - 28s 71ms/step - loss: 0.1915 - accuracy: 0.9459 - val_loss: 0.6416 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00121: Learning rate is 0.1000.\n",
      "Epoch 122/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1838 - accuracy: 0.9494 - val_loss: 0.6205 - val_accuracy: 0.8424\n",
      "\n",
      "Epoch 00122: Learning rate is 0.1000.\n",
      "Epoch 123/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1844 - accuracy: 0.9491 - val_loss: 0.6352 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 00123: Learning rate is 0.1000.\n",
      "Epoch 124/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1829 - accuracy: 0.9493 - val_loss: 0.5009 - val_accuracy: 0.8722\n",
      "\n",
      "Epoch 00124: Learning rate is 0.1000.\n",
      "Epoch 125/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1844 - accuracy: 0.9477 - val_loss: 0.7097 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00125: Learning rate is 0.1000.\n",
      "Epoch 126/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1830 - accuracy: 0.9494 - val_loss: 0.8864 - val_accuracy: 0.7696\n",
      "\n",
      "Epoch 00126: Learning rate is 0.1000.\n",
      "Epoch 127/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1812 - accuracy: 0.9500 - val_loss: 0.7295 - val_accuracy: 0.8287\n",
      "\n",
      "Epoch 00127: Learning rate is 0.1000.\n",
      "Epoch 128/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1819 - accuracy: 0.9486 - val_loss: 0.4811 - val_accuracy: 0.8740\n",
      "\n",
      "Epoch 00128: Learning rate is 0.1000.\n",
      "Epoch 129/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1795 - accuracy: 0.9496 - val_loss: 0.6010 - val_accuracy: 0.8472\n",
      "\n",
      "Epoch 00129: Learning rate is 0.1000.\n",
      "Epoch 130/300\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1756 - accuracy: 0.9506 - val_loss: 0.4727 - val_accuracy: 0.8778\n",
      "\n",
      "Epoch 00130: Learning rate is 0.1000.\n",
      "Epoch 131/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1787 - accuracy: 0.9495 - val_loss: 0.4727 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00131: Learning rate is 0.1000.\n",
      "Epoch 132/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1783 - accuracy: 0.9500 - val_loss: 0.5308 - val_accuracy: 0.8659\n",
      "\n",
      "Epoch 00132: Learning rate is 0.1000.\n",
      "Epoch 133/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1780 - accuracy: 0.9506 - val_loss: 0.5263 - val_accuracy: 0.8691\n",
      "\n",
      "Epoch 00133: Learning rate is 0.1000.\n",
      "Epoch 134/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1772 - accuracy: 0.9506 - val_loss: 0.4849 - val_accuracy: 0.8758\n",
      "\n",
      "Epoch 00134: Learning rate is 0.1000.\n",
      "Epoch 135/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1790 - accuracy: 0.9505 - val_loss: 0.5096 - val_accuracy: 0.8691\n",
      "\n",
      "Epoch 00135: Learning rate is 0.1000.\n",
      "Epoch 136/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1765 - accuracy: 0.9517 - val_loss: 0.6417 - val_accuracy: 0.8392\n",
      "\n",
      "Epoch 00136: Learning rate is 0.1000.\n",
      "Epoch 137/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1752 - accuracy: 0.9507 - val_loss: 0.4980 - val_accuracy: 0.8707\n",
      "\n",
      "Epoch 00137: Learning rate is 0.1000.\n",
      "Epoch 138/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1752 - accuracy: 0.9524 - val_loss: 0.6308 - val_accuracy: 0.8438\n",
      "\n",
      "Epoch 00138: Learning rate is 0.1000.\n",
      "Epoch 139/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1706 - accuracy: 0.9540 - val_loss: 0.5119 - val_accuracy: 0.8721\n",
      "\n",
      "Epoch 00139: Learning rate is 0.1000.\n",
      "Epoch 140/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1672 - accuracy: 0.9540 - val_loss: 0.5749 - val_accuracy: 0.8639\n",
      "\n",
      "Epoch 00140: Learning rate is 0.1000.\n",
      "Epoch 141/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1704 - accuracy: 0.9519 - val_loss: 0.5402 - val_accuracy: 0.8664\n",
      "\n",
      "Epoch 00141: Learning rate is 0.1000.\n",
      "Epoch 142/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1691 - accuracy: 0.9533 - val_loss: 0.6634 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00142: Learning rate is 0.1000.\n",
      "Epoch 143/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1719 - accuracy: 0.9520 - val_loss: 0.6832 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00143: Learning rate is 0.1000.\n",
      "Epoch 144/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1715 - accuracy: 0.9526 - val_loss: 0.5692 - val_accuracy: 0.8599\n",
      "\n",
      "Epoch 00144: Learning rate is 0.1000.\n",
      "Epoch 145/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1693 - accuracy: 0.9535 - val_loss: 0.7772 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00145: Learning rate is 0.1000.\n",
      "Epoch 146/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1689 - accuracy: 0.9539 - val_loss: 0.5510 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00146: Learning rate is 0.1000.\n",
      "Epoch 147/300\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.1681 - accuracy: 0.9532 - val_loss: 0.4545 - val_accuracy: 0.8801\n",
      "\n",
      "Epoch 00147: Learning rate is 0.1000.\n",
      "Epoch 148/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1633 - accuracy: 0.9554 - val_loss: 0.5060 - val_accuracy: 0.8699\n",
      "\n",
      "Epoch 00148: Learning rate is 0.1000.\n",
      "Epoch 149/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1690 - accuracy: 0.9537 - val_loss: 0.5888 - val_accuracy: 0.8564\n",
      "\n",
      "Epoch 00149: Learning rate is 0.1000.\n",
      "Epoch 150/300\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1651 - accuracy: 0.9551 - val_loss: 0.5655 - val_accuracy: 0.8616\n",
      "\n",
      "Epoch 00150: Learning rate is 0.0100.\n",
      "Epoch 151/300\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9683\n",
      "Reached 90.00% accuracy, so stopping training!!\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1290 - accuracy: 0.9683 - val_loss: 0.3780 - val_accuracy: 0.9045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3a2428ba8>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train) // batch_size, epochs=nb_epoch,\n",
    "                    callbacks=[CustomLearningRateScheduler(lr_schedule)],\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    validation_steps=X_test.shape[0] // batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "x6HFjtG0s1jl",
    "outputId": "4933f752-bab0-4bcf-a14b-cda9f9d1ef75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.37801840901374817\n",
      "Test accuracy: 0.9045000076293945\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "em9IvgV4s1jt",
    "outputId": "6b446287-9522-4108-bce0-d1e5df8055a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
